{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7581613,"sourceType":"datasetVersion","datasetId":4413391},{"sourceId":7704791,"sourceType":"datasetVersion","datasetId":4413206}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-27T12:21:25.728175Z","iopub.execute_input":"2024-02-27T12:21:25.728456Z","iopub.status.idle":"2024-02-27T12:21:26.646548Z","shell.execute_reply.started":"2024-02-27T12:21:25.728431Z","shell.execute_reply":"2024-02-27T12:21:26.645620Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nyaya-drishti1/dark_pattern1.csv\n/kaggle/input/nyaya-drishti1/dp_best.csv\n/kaggle/input/dark-pattern/dataset.tsv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 --progress-bar off\n!pip install -q peft==0.4.0 --progress-bar off\n!pip install -q bitsandbytes==0.40.2 --progress-bar off\n!pip install -q transformers==4.31.0 --progress-bar off\n!pip install -q trl==0.4.7 --progress-bar off\n!pip install transformers\n!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:21:26.648236Z","iopub.execute_input":"2024-02-27T12:21:26.648599Z","iopub.status.idle":"2024-02-27T12:23:06.817967Z","shell.execute_reply.started":"2024-02-27T12:21:26.648576Z","shell.execute_reply":"2024-02-27T12:23:06.817028Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom random import randrange\nfrom functools import partial\nimport torch\n\nfrom transformers import (AutoModelForCausalLM,\n                          AutoTokenizer,\n                          BitsAndBytesConfig,\n                          HfArgumentParser,\n                          Trainer,\n                          TrainingArguments,\n                          DataCollatorForLanguageModeling,\n                          EarlyStoppingCallback,\n                          pipeline,\n                          logging,\n                          set_seed)\n\nimport bitsandbytes as bnb\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\nfrom trl import SFTTrainer\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:23:06.819433Z","iopub.execute_input":"2024-02-27T12:23:06.819792Z","iopub.status.idle":"2024-02-27T12:23:27.145240Z","shell.execute_reply.started":"2024-02-27T12:23:06.819755Z","shell.execute_reply":"2024-02-27T12:23:27.144279Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-02-27 12:23:13.809359: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-27 12:23:13.809454: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-27 12:23:13.934644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n    \"\"\"\n    Configures model quantization method using bitsandbytes to speed up training and inference\n\n    :param load_in_4bit: Load model in 4-bit precision mode\n    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n    \"\"\"\n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit = load_in_4bit,\n        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n        bnb_4bit_quant_type = bnb_4bit_quant_type,\n        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n    )\n\n    return bnb_config","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:23:27.147631Z","iopub.execute_input":"2024-02-27T12:23:27.148228Z","iopub.status.idle":"2024-02-27T12:23:27.154091Z","shell.execute_reply.started":"2024-02-27T12:23:27.148203Z","shell.execute_reply":"2024-02-27T12:23:27.152858Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\ndef load_model(model_name, bnb_config):\n    \"\"\"\n    Loads model and model tokenizer\n\n    :param model_name: Hugging Face model name\n    :param bnb_config: Bitsandbytes configuration\n    \"\"\"\n\n    # Get number of GPU device and set maximum memory\n    n_gpus = torch.cuda.device_count()\n    max_memory = f'{40960}MB'\n\n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config = bnb_config,\n        device_map = \"auto\", # dispatch the model efficiently on the available resources\n        max_memory = {i: max_memory for i in range(n_gpus)},\n    )\n\n    # Load model tokenizer with the user authentication token\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = False)\n\n    # Set padding token as EOS token\n    tokenizer.pad_token = tokenizer.eos_token\n\n    return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:23:27.155208Z","iopub.execute_input":"2024-02-27T12:23:27.155495Z","iopub.status.idle":"2024-02-27T12:23:27.166814Z","shell.execute_reply.started":"2024-02-27T12:23:27.155471Z","shell.execute_reply":"2024-02-27T12:23:27.165938Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# transformers parameters\n################################################################################\n\n# The pre-trained model from the Hugging Face Hub to load and fine-tune\nmodel_name = \"daryl149/llama-2-7b-chat-hf\"\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nload_in_4bit = True\n\n# Activate nested quantization for 4-bit base models (double quantization)\nbnb_4bit_use_double_quant = True\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Compute data type for 4-bit base models\nbnb_4bit_compute_dtype = torch.bfloat16","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:23:27.167894Z","iopub.execute_input":"2024-02-27T12:23:27.168124Z","iopub.status.idle":"2024-02-27T12:23:27.179466Z","shell.execute_reply.started":"2024-02-27T12:23:27.168104Z","shell.execute_reply":"2024-02-27T12:23:27.178771Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load model from Hugging Face Hub with model name and bitsandbytes configuration\n\nbnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n\nmodel, tokenizer = load_model(model_name, bnb_config)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:23:27.180449Z","iopub.execute_input":"2024-02-27T12:23:27.180701Z","iopub.status.idle":"2024-02-27T12:25:09.586691Z","shell.execute_reply.started":"2024-02-27T12:23:27.180680Z","shell.execute_reply":"2024-02-27T12:25:09.585881Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e96b4abe0cf24a2f8eb55bfc86aee8f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efef40b42d0c42c2af3fcaf41b1730da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b04b8aabc104611a15a2862c7d69858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff3485f3093c4659a2395c656c6ecb6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e71deba06c9406b864e634cf4ab9168"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a918283c8f7142b980d86ae48c49d3fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0e0182ebf8649cb8d18719f19a6056f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c68ba047a7294a639bf19e08f17076d4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37adf66f39b44b3c835301f11f204137"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5748654168849d1a22972f96236042c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a68137391ffc4998a5262f64d4941329"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfile_path = '/kaggle/input/dark-pattern/dataset.tsv'\ndf = pd.read_csv(file_path, sep='\\t')\n\ndf = df.drop('page_id', axis=1)\ndf = df.drop('label', axis=1)\n\ndf['instruction']=\"\"\"categorize the dark pattern in website into one of 8 categories:\n\nUrgency\nNot Dark Pattern\nScarcity\nMisdirection\nSocial Proof\nObstruction\nSneaking\nForced Action\n\"\"\"\n\ndf = df.rename(columns={'text': 'input'})\ndf = df.rename(columns={'Pattern Category': 'output'})\n\noriginal_columns = df.columns.tolist()\n\ndesired_column_order = ['instruction', 'input', 'output']  # Replace with your actual column names and order\ndf= df[desired_column_order]\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.587750Z","iopub.execute_input":"2024-02-27T12:25:09.588024Z","iopub.status.idle":"2024-02-27T12:25:09.632130Z","shell.execute_reply.started":"2024-02-27T12:25:09.588001Z","shell.execute_reply":"2024-02-27T12:25:09.631213Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.to_csv('dark_pattern1.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.633290Z","iopub.execute_input":"2024-02-27T12:25:09.633618Z","iopub.status.idle":"2024-02-27T12:25:09.665860Z","shell.execute_reply.started":"2024-02-27T12:25:09.633587Z","shell.execute_reply":"2024-02-27T12:25:09.665189Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n# Load dataset\ndataset = Dataset.from_pandas(df,split=\"train\")\n# dataset = load_dataset(\"csv\", data_files = '/kaggle/input/nyaya-drishti1/dark_pattern1.csv', split = \"train\")","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.668926Z","iopub.execute_input":"2024-02-27T12:25:09.669170Z","iopub.status.idle":"2024-02-27T12:25:09.697812Z","shell.execute_reply.started":"2024-02-27T12:25:09.669150Z","shell.execute_reply":"2024-02-27T12:25:09.696984Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.698722Z","iopub.execute_input":"2024-02-27T12:25:09.698989Z","iopub.status.idle":"2024-02-27T12:25:09.705044Z","shell.execute_reply.started":"2024-02-27T12:25:09.698967Z","shell.execute_reply":"2024-02-27T12:25:09.704193Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 2356\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(f'Number of prompts: {len(dataset)}')\nprint(f'Column names are: {dataset.column_names}')","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.706184Z","iopub.execute_input":"2024-02-27T12:25:09.706498Z","iopub.status.idle":"2024-02-27T12:25:09.714203Z","shell.execute_reply.started":"2024-02-27T12:25:09.706467Z","shell.execute_reply":"2024-02-27T12:25:09.713335Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Number of prompts: 2356\nColumn names are: ['instruction', 'input', 'output']\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset[randrange(len(dataset))]","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.715582Z","iopub.execute_input":"2024-02-27T12:25:09.715861Z","iopub.status.idle":"2024-02-27T12:25:09.726863Z","shell.execute_reply.started":"2024-02-27T12:25:09.715813Z","shell.execute_reply":"2024-02-27T12:25:09.726042Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'instruction': 'categorize the dark pattern in website into one of 8 categories:\\n\\nUrgency\\nNot Dark Pattern\\nScarcity\\nMisdirection\\nSocial Proof\\nObstruction\\nSneaking\\nForced Action\\n',\n 'input': 'Perhaps you could...',\n 'output': 'Not Dark Pattern'}"},"metadata":{}}]},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Creates a formatted prompt template for a prompt in the instruction dataset\n\n    :param sample: Prompt or sample from the instruction dataset\n    \"\"\"\n\n    # Initialize static strings for the prompt template\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruction:\"\n    INPUT_KEY = \"Input:\"\n    RESPONSE_KEY = \"### Response:\"\n    END_KEY = \"### End\"\n\n    # Combine a prompt with the static strings\n    blurb = f\"{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n    input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n    end = f\"{END_KEY}\"\n\n    # Create a list of prompt template elements\n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    # Join prompt template elements into a single string to create the prompt template\n    formatted_prompt = \"\\n\\n\".join(parts)\n\n    # Store the formatted prompt template in a new key \"text\"\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.727981Z","iopub.execute_input":"2024-02-27T12:25:09.728494Z","iopub.status.idle":"2024-02-27T12:25:09.735811Z","shell.execute_reply.started":"2024-02-27T12:25:09.728464Z","shell.execute_reply":"2024-02-27T12:25:09.734970Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"create_prompt_formats(dataset[randrange(len(dataset))])","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.737119Z","iopub.execute_input":"2024-02-27T12:25:09.737378Z","iopub.status.idle":"2024-02-27T12:25:09.750368Z","shell.execute_reply.started":"2024-02-27T12:25:09.737356Z","shell.execute_reply":"2024-02-27T12:25:09.749565Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'instruction': 'categorize the dark pattern in website into one of 8 categories:\\n\\nUrgency\\nNot Dark Pattern\\nScarcity\\nMisdirection\\nSocial Proof\\nObstruction\\nSneaking\\nForced Action\\n',\n 'input': 'Delivery & Returns',\n 'output': 'Not Dark Pattern',\n 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\ncategorize the dark pattern in website into one of 8 categories:\\n\\nUrgency\\nNot Dark Pattern\\nScarcity\\nMisdirection\\nSocial Proof\\nObstruction\\nSneaking\\nForced Action\\n\\n\\nInput:\\nDelivery & Returns\\n\\n### Response:\\nNot Dark Pattern\\n\\n### End'}"},"metadata":{}}]},{"cell_type":"code","source":"def get_max_length(model):\n    \"\"\"\n    Extracts maximum token length from the model configuration\n\n    :param model: Hugging Face model\n    \"\"\"\n\n    # Pull model configuration\n    conf = model.config\n    # Initialize a \"max_length\" variable to store maximum sequence length as null\n    max_length = None\n    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.751491Z","iopub.execute_input":"2024-02-27T12:25:09.751816Z","iopub.status.idle":"2024-02-27T12:25:09.760394Z","shell.execute_reply.started":"2024-02-27T12:25:09.751787Z","shell.execute_reply":"2024-02-27T12:25:09.759656Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizes dataset batch\n\n    :param batch: Dataset batch\n    :param tokenizer: Model tokenizer\n    :param max_length: Maximum number of tokens to emit from the tokenizer\n    \"\"\"\n\n    return tokenizer(\n        batch[\"text\"],\n        max_length = max_length,\n        truncation = True,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.761413Z","iopub.execute_input":"2024-02-27T12:25:09.761984Z","iopub.status.idle":"2024-02-27T12:25:09.774274Z","shell.execute_reply.started":"2024-02-27T12:25:09.761954Z","shell.execute_reply":"2024-02-27T12:25:09.773564Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n    \"\"\"\n    Tokenizes dataset for fine-tuning\n\n    :param tokenizer (AutoTokenizer): Model tokenizer\n    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n    :param seed: Random seed for reproducibility\n    :param dataset (str): Instruction dataset\n    \"\"\"\n\n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)\n\n    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched = True,\n        remove_columns = [\"instruction\", \"input\", \"output\", \"text\"],\n    )\n\n    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n\n    # Shuffle dataset\n    dataset = dataset.shuffle(seed = seed)\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.775467Z","iopub.execute_input":"2024-02-27T12:25:09.775984Z","iopub.status.idle":"2024-02-27T12:25:09.784614Z","shell.execute_reply.started":"2024-02-27T12:25:09.775954Z","shell.execute_reply":"2024-02-27T12:25:09.783889Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Random seed\nseed = 33\n\nmax_length = get_max_length(model)\npreprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:09.785516Z","iopub.execute_input":"2024-02-27T12:25:09.785790Z","iopub.status.idle":"2024-02-27T12:25:10.846481Z","shell.execute_reply.started":"2024-02-27T12:25:09.785768Z","shell.execute_reply":"2024-02-27T12:25:10.845637Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Found max lenth: 2048\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2356 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ecfbfabd7c04220bdb0c0231130fd60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"847f9177616b4df090b3cda0e9054383"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22e97a1a48b2482b8db616c63527bf11"}},"metadata":{}}]},{"cell_type":"code","source":"print(preprocessed_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:10.847577Z","iopub.execute_input":"2024-02-27T12:25:10.847885Z","iopub.status.idle":"2024-02-27T12:25:10.852492Z","shell.execute_reply.started":"2024-02-27T12:25:10.847855Z","shell.execute_reply":"2024-02-27T12:25:10.851610Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 2356\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n    \"\"\"\n    Creates Parameter-Efficient Fine-Tuning configuration for the model\n\n    :param r: LoRA attention dimension\n    :param lora_alpha: Alpha parameter for LoRA scaling\n    :param modules: Names of the modules to apply LoRA to\n    :param lora_dropout: Dropout Probability for LoRA layers\n    :param bias: Specifies if the bias parameters should be trained\n    \"\"\"\n    config = LoraConfig(\n        r = r,\n        lora_alpha = lora_alpha,\n        target_modules = target_modules,\n        lora_dropout = lora_dropout,\n        bias = bias,\n        task_type = task_type,\n    )\n\n    return config","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:10.853758Z","iopub.execute_input":"2024-02-27T12:25:10.854052Z","iopub.status.idle":"2024-02-27T12:25:10.862262Z","shell.execute_reply.started":"2024-02-27T12:25:10.854029Z","shell.execute_reply":"2024-02-27T12:25:10.861414Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def find_all_linear_names(model):\n    \"\"\"\n    Find modules to apply LoRA to.\n\n    :param model: PEFT model\n    \"\"\"\n\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    print(f\"LoRA module names: {list(lora_module_names)}\")\n    return list(lora_module_names)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:10.863219Z","iopub.execute_input":"2024-02-27T12:25:10.863467Z","iopub.status.idle":"2024-02-27T12:25:10.873017Z","shell.execute_reply.started":"2024-02-27T12:25:10.863446Z","shell.execute_reply":"2024-02-27T12:25:10.872198Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model, use_4bit = False):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n\n    :param model: PEFT model\n    \"\"\"\n\n    trainable_params = 0\n    all_param = 0\n\n    for _, param in model.named_parameters():\n        num_params = param.numel()\n        if num_params == 0 and hasattr(param, \"ds_numel\"):\n            num_params = param.ds_numel\n        all_param += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n\n    if use_4bit:\n        trainable_params /= 2\n\n    print(\n        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:10.874167Z","iopub.execute_input":"2024-02-27T12:25:10.874509Z","iopub.status.idle":"2024-02-27T12:25:10.886323Z","shell.execute_reply.started":"2024-02-27T12:25:10.874479Z","shell.execute_reply":"2024-02-27T12:25:10.885581Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def fine_tune(model,\n          tokenizer,\n          dataset,\n          lora_r,\n          lora_alpha,\n          lora_dropout,\n          bias,\n          task_type,\n          per_device_train_batch_size,\n          gradient_accumulation_steps,\n          warmup_steps,\n          max_steps,\n          learning_rate,\n          fp16,\n          logging_steps,\n          output_dir,\n          optim):\n    \"\"\"\n    Prepares and fine-tune the pre-trained model.\n\n    :param model: Pre-trained Hugging Face model\n    :param tokenizer: Model tokenizer\n    :param dataset: Preprocessed training dataset\n    \"\"\"\n\n    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n    model.gradient_checkpointing_enable()\n\n    # Prepare the model for training\n    model = prepare_model_for_kbit_training(model)\n\n    # Get LoRA module names\n    target_modules = find_all_linear_names(model)\n\n    # Create PEFT configuration for these modules and wrap the model to PEFT\n    peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n    model = get_peft_model(model, peft_config)\n\n    # Print information about the percentage of trainable parameters\n    print_trainable_parameters(model)\n\n    # Training parameters\n    trainer = Trainer(\n        model = model,\n        train_dataset = dataset,\n        args = TrainingArguments(\n            per_device_train_batch_size = per_device_train_batch_size,\n            gradient_accumulation_steps = gradient_accumulation_steps,\n            warmup_steps = warmup_steps,\n            max_steps = max_steps,\n            learning_rate = learning_rate,\n            fp16 = fp16,\n            logging_steps = logging_steps,\n            output_dir = output_dir,\n            optim = optim,\n        ),\n        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n    )\n\n    model.config.use_cache = False\n\n    do_train = True\n\n    # Launch training and log metrics\n    print(\"Training...\")\n\n    if do_train:\n        train_result = trainer.train()\n        metrics = train_result.metrics\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n        print(metrics)\n\n    # Save model\n    print(\"Saving last checkpoint of the model...\")\n    os.makedirs(output_dir, exist_ok = True)\n    trainer.model.save_pretrained(output_dir)\n\n    # Free memory for merging weights\n    del model\n    del trainer\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:10.887460Z","iopub.execute_input":"2024-02-27T12:25:10.887985Z","iopub.status.idle":"2024-02-27T12:25:10.899178Z","shell.execute_reply.started":"2024-02-27T12:25:10.887955Z","shell.execute_reply":"2024-02-27T12:25:10.898275Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 16\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 64\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n# Bias\nbias = \"none\"\n\n# Task type\ntask_type = \"CAUSAL_LM\"","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:10.900497Z","iopub.execute_input":"2024-02-27T12:25:10.900788Z","iopub.status.idle":"2024-02-27T12:25:10.912349Z","shell.execute_reply.started":"2024-02-27T12:25:10.900765Z","shell.execute_reply":"2024-02-27T12:25:10.911625Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Batch size per GPU for training\nper_device_train_batch_size = 1\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 4\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = 10\n\n# Linear warmup steps from 0 to learning_rate\nwarmup_steps = 2\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = True\n\n# Log every X updates steps\nlogging_steps = 1","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:10.913492Z","iopub.execute_input":"2024-02-27T12:25:10.913792Z","iopub.status.idle":"2024-02-27T12:25:10.925661Z","shell.execute_reply.started":"2024-02-27T12:25:10.913759Z","shell.execute_reply":"2024-02-27T12:25:10.924912Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"fine_tune(model,\n      tokenizer,\n      preprocessed_dataset,\n      lora_r,\n      lora_alpha,\n      lora_dropout,\n      bias,\n      task_type,\n      per_device_train_batch_size,\n      gradient_accumulation_steps,\n      warmup_steps,\n      max_steps,\n      learning_rate,\n      fp16,\n      logging_steps,\n      output_dir,\n      optim)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:25:10.926636Z","iopub.execute_input":"2024-02-27T12:25:10.926964Z","iopub.status.idle":"2024-02-27T12:36:10.129435Z","shell.execute_reply.started":"2024-02-27T12:25:10.926940Z","shell.execute_reply":"2024-02-27T12:36:10.128411Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"LoRA module names: ['up_proj', 'gate_proj', 'down_proj', 'o_proj', 'q_proj', 'v_proj', 'k_proj']\nAll Parameters: 3,540,389,888 || Trainable Parameters: 39,976,960 || Trainable Parameters %: 1.1291682911958425\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240227_123459-rjwxq39o</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cse200001003/huggingface/runs/rjwxq39o' target=\"_blank\">cosmic-serenity-5</a></strong> to <a href='https://wandb.ai/cse200001003/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cse200001003/huggingface' target=\"_blank\">https://wandb.ai/cse200001003/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cse200001003/huggingface/runs/rjwxq39o' target=\"_blank\">https://wandb.ai/cse200001003/huggingface/runs/rjwxq39o</a>"},"metadata":{}},{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:30, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.634500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.618500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.662400</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.949200</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.642700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"***** train metrics *****\n  epoch                    =       0.01\n  total_flos               =    42960GF\n  train_loss               =     2.7015\n  train_runtime            = 0:09:53.50\n  train_samples_per_second =      0.034\n  train_steps_per_second   =      0.008\n{'train_runtime': 593.5021, 'train_samples_per_second': 0.034, 'train_steps_per_second': 0.008, 'total_flos': 46128071024640.0, 'train_loss': 2.701456570625305, 'epoch': 0.01}\nSaving last checkpoint of the model...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load fine-tuned weights\nmodel = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map = \"auto\", torch_dtype = torch.bfloat16)\n# Merge the LoRA layers with the base model\nmodel = model.merge_and_unload()\n\n# Save fine-tuned model at a new location\noutput_merged_dir = \"results/dp/final_merged_checkpoint\"\nos.makedirs(output_merged_dir, exist_ok = True)\nmodel.save_pretrained(output_merged_dir, safe_serialization = True)\n\n# Save tokenizer for easy inference\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained(output_merged_dir)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:36:10.134998Z","iopub.execute_input":"2024-02-27T12:36:10.135633Z","iopub.status.idle":"2024-02-27T12:42:11.985354Z","shell.execute_reply.started":"2024-02-27T12:36:10.135603Z","shell.execute_reply":"2024-02-27T12:42:11.983234Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0541f2c2fef4bf0b4c0cab4a608a219"}},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"('results/dp/final_merged_checkpoint/tokenizer_config.json',\n 'results/dp/final_merged_checkpoint/special_tokens_map.json',\n 'results/dp/final_merged_checkpoint/tokenizer.model',\n 'results/dp/final_merged_checkpoint/added_tokens.json',\n 'results/dp/final_merged_checkpoint/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"instruction = \"Classify into specific label of dark pattern\"\n  # Enter your own instruction here.\ninput_query = (\n    \"2 left in stock\"  # Write additional context for the model here.\n)\n\ninputs = tokenizer(instruction, input_query, return_tensors=\"pt\")\n%time outputs = model.generate(**inputs,max_new_tokens=20)\n\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-02-27T13:27:23.385557Z","iopub.execute_input":"2024-02-27T13:27:23.385860Z","iopub.status.idle":"2024-02-27T13:27:41.949229Z","shell.execute_reply.started":"2024-02-27T13:27:23.385806Z","shell.execute_reply":"2024-02-27T13:27:41.947446Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"CPU times: user 37.1 s, sys: 0 ns, total: 37.1 s\nWall time: 18.6 s\n['Classify into specific label of dark pattern 2 left in stock\\n\\n```\\n- Dark Pattern 2: Scarcity\\n- Dark Pattern 2:']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}