{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NWXtFB1Yz29C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "import time\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/dp_best.csv\",header=None)"
      ],
      "metadata": {
        "id": "2ogCq57X0Asy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing to lowercase\n",
        "df[0] = df[0].str.lower()\n",
        "\n",
        "# removing urls\n",
        "df[0] = df[0].str.replace('http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "# removing new lines \"\\n\"\n",
        "df[0] = df[0].str.replace('\\n',' ', regex=True)\n",
        "\n",
        "# removing all the punctuations\n",
        "df[0] = df[0].str.replace('[^\\w\\s]',' ')\n",
        "\n",
        "# removing integers\n",
        "df[0] = df[0].str.replace('\\d','', regex=True)\n",
        "\n",
        "# removing emojis\n",
        "df[0] = df[0].str.replace('[^\\w\\s#@/:%.,_-]', ' ', flags=re.UNICODE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ-I82Af0GbU",
        "outputId": "95ae2027-27f9-454f-ee52-819ef6cdb9b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-cdd03867b11b>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[0] = df[0].str.replace('http\\S+|www.\\S+', '', case=False)\n",
            "<ipython-input-4-cdd03867b11b>:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[0] = df[0].str.replace('[^\\w\\s]',' ')\n",
            "<ipython-input-4-cdd03867b11b>:17: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[0] = df[0].str.replace('[^\\w\\s#@/:%.,_-]', ' ', flags=re.UNICODE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def cleaning_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
        "\n",
        "df[0] = df[0].apply(lambda text: cleaning_stopwords(text))\n",
        "\n",
        "df[0].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PiSuZZ10M_e",
        "outputId": "3c7f05ee-be4d-440b-9baa-1d45b1dc2bd8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0               thanks like deals\n",
              "1           rather pay full price\n",
              "2                  like discounts\n",
              "3         thanks like great deals\n",
              "4    thanks rather pay full price\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    return lemmatizer.lemmatize(text)\n",
        "\n",
        "df[0] = df[0].apply(lemmatize_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SehTomNj0RL3",
        "outputId": "c050fc3d-25a0-4a72-ec21-f9de5cff0e66"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_mapping = {\n",
        "    'Shaming': 0,\n",
        "    'False Urgency': 1,\n",
        "    'Nagging': 2,\n",
        "    'Subscription Trap': 3,\n",
        "    'Basket Sneaking': 4,\n",
        "    'Not Dark Pattern':5\n",
        "}\n",
        "df[1] = df[1].map(sentiment_mapping)"
      ],
      "metadata": {
        "id": "aVZ48Fnx0SO9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=500, split=' ')\n",
        "tokenizer.fit_on_texts(df[0].values)\n",
        "X_learn = tokenizer.texts_to_sequences(df[0].values)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "X_learn = pad_sequences(X_learn)"
      ],
      "metadata": {
        "id": "wHwAPAjl0Ubf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_learn=pd.get_dummies(df[1])"
      ],
      "metadata": {
        "id": "rgiS9a050Wuj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_learn, y_learn, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "34qFiUZx0Y-u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7OZ0Ani0gaD",
        "outputId": "507e64ec-4cf3-4e95-8ded-300a2ecbdc14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = dict()\n",
        "f = open('/content/drive/MyDrive/glove.6B.200d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz8VkxYN0bEk",
        "outputId": "98febeda-5e2e-4d39-9ecf-7aee20cf79cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.random.random((len(word_index)+1,200))\n",
        "for word, i in word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "k9cFLbJz0dKX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 200,weights=[embedding_matrix], input_shape = X_train[0].shape))\n",
        "model.add(SpatialDropout1D(0.002))\n",
        "model.add(LSTM(100,return_sequences=True))\n",
        "model.add(LSTM(80,return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(6,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6Y_qGqq0x9f",
        "outputId": "0bfc930d-68c8-41e9-fdbc-7a08bc7d8d14"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 56, 200)           578400    \n",
            "                                                                 \n",
            " spatial_dropout1d_1 (Spati  (None, 56, 200)           0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 56, 100)           120400    \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 56, 80)            57920     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 50)                26200     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 306       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 783226 (2.99 MB)\n",
            "Trainable params: 783226 (2.99 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "model.fit(X_train, y_train, epochs =20,steps_per_epoch=500, batch_size=batch_size, verbose = 'auto')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utY7Ym4K00Nm",
        "outputId": "80425a0f-51ae-4b82-eff2-b699825d19f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "500/500 [==============================] - 82s 149ms/step - loss: 0.3631 - accuracy: 0.8789\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 70s 139ms/step - loss: 0.1404 - accuracy: 0.9646\n",
            "Epoch 3/20\n",
            "340/500 [===================>..........] - ETA: 22s - loss: 0.1239 - accuracy: 0.9679"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r500/500 [==============================] - 48s 95ms/step - loss: 0.1239 - accuracy: 0.9679\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dcd6fb20820>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(X_test)\n",
        "y_pred=np.argmax(y_pred,axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt2R37Db06tT",
        "outputId": "3ad7431f-23be-43d1-dad7-801c31739544"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 2s 38ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test=y_test.to_numpy()\n",
        "y_test=np.argmax(y_test,axis=1)"
      ],
      "metadata": {
        "id": "u1Ujfq8R1Clt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Print the precision and recall, among other metrics\n",
        "print(metrics.classification_report(y_test, y_pred, digits=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1djo8AId1Ftt",
        "outputId": "e9997eae-0750-49e9-e3de-55bacaf9f45e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 28   9   0   0   0   9]\n",
            " [  2 239   0   0   0   2]\n",
            " [  0   1   1   0   0   1]\n",
            " [  0   1   2   3   0   1]\n",
            " [  0   0   0   1   1   0]\n",
            " [  3   6   0   0   0 225]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.84848   0.60870   0.70886        46\n",
            "           1    0.93359   0.98354   0.95792       243\n",
            "           2    0.33333   0.33333   0.33333         3\n",
            "           3    0.75000   0.42857   0.54545         7\n",
            "           4    1.00000   0.50000   0.66667         2\n",
            "           5    0.94538   0.96154   0.95339       234\n",
            "\n",
            "    accuracy                        0.92897       535\n",
            "   macro avg    0.80180   0.63595   0.69427       535\n",
            "weighted avg    0.92591   0.92897   0.92453       535\n",
            "\n"
          ]
        }
      ]
    }
  ]
}